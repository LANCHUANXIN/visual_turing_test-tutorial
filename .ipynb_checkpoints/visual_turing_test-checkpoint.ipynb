{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Turing Test - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Scalable Learning and Perception Group](http://scalable.mpi-inf.mpg.de), authored by [Dr. Mario Fritz](http://scalable.mpi-inf.mpg.de/publications/) and [Mateusz Malinowski](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/mateusz-malinowski/). We are also curious about your opinion on the tutorial; every feedback is welcomed, please write to mmalinow@mpi-inf.mpg.de."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on our ICCV'15 paper [\"Ask Your Neurons: A Neural-based Approach to Answering Questions about Images\"](https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf), and, more broadly, our project on [Visual Turing Test](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since visual features are large, you should download the features separately, and put them to data/visual_features/visual_features or data/vqa/visual_features directory.\n",
    " * daquar (it is recommended to download all them)\n",
    "  * [residual net [23 MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/daquar/fb_resnet.zip)\n",
    "   -- place under: data/daquar/visual_features/fb_resnet/blobs.*.npy\n",
    "  * [googlenet [17 MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/daquar/googlenet.zip)\n",
    "   -- place under: data/daquar/visual_features/googlenet/blobs.*.npy\n",
    "  * [NYU-Depth images [430 MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16language_vision_tutorial/nyu_depth-images.zip)\n",
    "   -- place under: data/daquar/images/*.png\n",
    " * vqa\n",
    "  * [residual_net train [1.2GB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/vqa/train2014/fb_resnet.zip) \n",
    "   -- recommended, place under: data/vqa/visual_features/train2014/fb_resnet/blobs.*.npy\n",
    "  * [residual_net val [573MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/vqa/val2014/fb_resnet.zip) \n",
    "   -- recommended, place under: data/vqa/visual_features/val2014/fb_resnet/blobs.*.npy\n",
    "  * [vgg_net train [2.2GB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/vqa/train2014/vgg_net.zip)\n",
    "   -- a few different visual features, place under: data/vqa/visual_features/train2014/vgg_net/blobs.*.npy\n",
    "  * [vgg_net val [1.3GB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16visual_features/vqa/val2014/vgg_net.zip)\n",
    "   -- a few different visual features, place under: data/vqa/visual_features/val2014/vgg_net/blobs.*.npy\n",
    " * vqa - question answer pairs\n",
    "  * [Questions [134MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16language_vision_tutorial/Questions.zip) -- place under: data/vqa/Questions/\n",
    "  * [Annotations [56MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16language_vision_tutorial/Annotations.zip) -- place under: data/vqa/Annotations\n",
    " * nltk data [recommended]\n",
    "  * [nltk_data [20MB]](http://datasets.d2.mpi-inf.mpg.de/mateusz16language_vision_tutorial/nltk_data.zip)\n",
    "   -- place under: data/nltk_data\n",
    " * tutorial\n",
    "  * this version of tutorial can be found [here](http://datasets.d2.mpi-inf.mpg.de/mateusz16language_vision_tutorial/visual_turing_test-tutorial.zip)\n",
    "  * [github version](https://github.com/mateuszmalinowski/visual_turing_test-tutorial) -- this version will be further updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use this tutorial or its parts or Kraino in your project, please consider citing at least our 'Ask Your Neurons' paper (you can find the bibtex below).\n",
    "\n",
    "Bibtex:\n",
    "\n",
    "```\n",
    "@article{malinowski2016ask\n",
    "  title={Ask your neurons: A Deep Learning Approach to Visual Question Answering},\n",
    "  author={Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},\n",
    "  booktitle={arXiv preprint arXiv:1605.02697},\n",
    "  year={2016}\n",
    "}\n",
    "or\n",
    "@inproceedings{malinowski2015ask,\n",
    "  title={Ask your neurons: A neural-based approach to answering questions about images},\n",
    "  author={Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},\n",
    "  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n",
    "  pages={1--9},\n",
    "  year={2015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the tutorial make sure that you have the following hierarchy of the folders:\n",
    "boring_function.py\n",
    "neural_solver.py\n",
    "visual_turing_test.ipynb\n",
    "data/*\n",
    "kraino/*\n",
    "local/*\n",
    "fig/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use this notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before focusing on the actual task, let's briefly see what we can do in the Jupyter Notebook. We will also introduce notation that we use in this tutorial.\n",
    "\n",
    "Shortcuts:\n",
    " * Shift + Enter - runs the cell, and step inside the next cell\n",
    " * Ctrl + Enter - runs the cell (stay in the same cell)\n",
    " * Esc + x - deletes the cell (be careful)\n",
    " * Esc + b - creates a cell bellow the current cell\n",
    "\n",
    "\n",
    "The following represents an exercise that doesn't need programming. Its role is to practice some newly introduced concepts.\n",
    "```\n",
    "Exercise\n",
    "```\n",
    "\n",
    "The following is a python script. \n",
    "```python\n",
    "print(\"Hello world\")\n",
    "\n",
    "# Comment\n",
    "# Now we write a loop printing numbers from 0 to 9\n",
    "for k in xrange(10):\n",
    "    # remember that the python's syntax is driven by indentation\n",
    "    print k\n",
    "```\n",
    "\n",
    "Some exercises need some programming, or at least executing the code.\n",
    "However, in this tutorial, we try to keep the programming part rather minimal, and focus on the Visual Turing Test.\n",
    "The following cell is a small programming exercise. You can edit it by double clicking the cell, and execute it by running the cell (Shift + Enter). We use #TODO: to give some hints or more detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_n_numbers(n):\n",
    "    #TODO: write a loop that prints numbers from 0 to n (excluding n)\n",
    "    for i in xrange(n):\n",
    "        print(i)\n",
    "\n",
    "# now we execute the function\n",
    "print_n_numbers(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below print each element in the list in a new line. We will use this function later, so please run the interpreter over the following cell (Shift+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_list(ll):\n",
    "    # Prints the list\n",
    "    print('\\n'.join(ll))\n",
    "    \n",
    "print_list(['Visual Turing Test', 'Summer School', 'Dr. Mario Fritz', 'Mateusz Malinowski'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook can also interface with the command line. Try the following line (again Shift+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's execute python's 'boring_function' with an argument. It prints the available GPU, the argument, as well as versions of Theano, and Keras (we will talk about both frameworks later in the tutorial). Since the boring_function imports Theano, its execution may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python boring_function.py 'hello world'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the content of the function, run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! tail boring_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below checks the available GPU machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this tutorial, we will look at very recent research thread that interlinks language and vision together -- a Visual Turing Test -- that is about answering on natural language questions about images by machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![challenges](fig/challenges.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Datasets](#Datasets)\n",
    "1. [Textual Features](#Textual-Features)\n",
    "1. [Language Only](#Language-Only)\n",
    "1. [Evaluation Measures](#Evaluation-Measures)\n",
    "1. [New Predictions](#New-Predictions)\n",
    "1. [Visual Features](#Visual-Features)\n",
    "1. [Vision+Language](#Vision+Language)\n",
    "1. [New Predictions with Vision+Language](#New Predictions-with-Vision+Language)\n",
    "1. [VQA](#VQA)\n",
    "1. [Kraino](#Kraino)\n",
    "1. [Further Experiments](#Further-Experiments)\n",
    "1. [New Research Opportunities](#New-Research-Opportunities)\n",
    "1. [External Links](#External-Links)\n",
    "1. [Logs](#Logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will get familiar with both datasets, accuracy measures, and features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAQUAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look into the folder *data/daquar* to make the problem a bit more tangible for us. Here, we have training and test data in *qa.894.raw.train.format_triple* and *qa.894.raw.test.format_triple*.\n",
    "\n",
    "```\n",
    "Execute the cell below to see how input data look like.\n",
    "Please use Shift+Enter on the cell below.\n",
    "Make sure you understand the format.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! head -15 data/daquar/qa.894.raw.train.format_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a look at the figure in [Introduction->Challenge](#Challenge). The figure lists images with associated question-answer pairs. It also comments on challenges associated with every question-answer-triplet. We see that to answer properly on the questions, the answerer needs to understand the scene visually, understand the question, but also, arguably, has to resort to common sense knowledge, or even know the preferences of the person asking a question ('What is behind the table?' - what 'behind' means?).\n",
    "\n",
    "```\n",
    "Can you see anything particularly interesting in the first column of the figure in [Introduction->Challenge]?\n",
    "Think about a spatial relationship between an observer, object of interest, and the world. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Execute the following procedure (Shift+Enter)\n",
    "from kraino.utils import data_provider\n",
    "\n",
    "dp = data_provider.select['daquar-triples']\n",
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above returns a dictionary of three representations of the DAQUAR dataset. For now, we will look only into the 'text' representation. dp['text'] returns a function from dataset split into the dataset's textual representation. It will be more clear after executing the following instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the keys of the representation of DAQUAR train\n",
    "train_text_representation = dp['text'](train_or_test='train')\n",
    "train_text_representation.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation specifies how questions are ended ('?'), answers are ended ('.'), answer words are delimited (DAQUAR sometimes has a set of answer words as an answer, for instance 'knife, fork' may be a valid answer), but most important, it has questions (key 'x'), answers (key 'y'), and names of the corresponding images (key 'img_name')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's check some entries of the text's representation\n",
    "n_elements = 10\n",
    "print('== Questions:')\n",
    "print_list(train_text_representation['x'][:n_elements])\n",
    "print\n",
    "print('== Answers:')\n",
    "print_list(train_text_representation['y'][:n_elements])\n",
    "print\n",
    "print('== Image Names:')\n",
    "print_list(train_text_representation['img_name'][:n_elements])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary__\n",
    "\n",
    "DAQUAR consists of question, answer, image triplets. Pairs question, answer for different folds are accessible from\n",
    "```python\n",
    "data_provider.select['text']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We have a text. But unfortunately neural networks expect numerical input, so we cannot really work with the raw text. We need to transform an raw input into some numerical value or a vector of values. One particularly successful representation is called one-hot vector and it is a binary vector with exactly one non-zero entry. This entry points to the corresponding word in the vocabulary. See the illustration below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one hot](fig/one_hot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "* How does a vector computed after we sum up one-hot representations of 'what is behind the table' (see the illustration above) look like?\n",
    "* What if we sum up 'What table is behind the table'? Can you interpret the resulting vector?\n",
    "* Can you guess why it's nice to work with one-hot vector representation of the text?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the illustrative example above, we first need to build a suitable vocabulary from our raw textual training data, and next transform them into one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from toolz import frequencies\n",
    "train_raw_x = train_text_representation['x']\n",
    "# we start from building the frequencies table\n",
    "wordcount_x = frequencies(' '.join(train_raw_x).split(' '))\n",
    "# print the most and least frequent words\n",
    "n_show = 5\n",
    "print(sorted(wordcount_x.items(), key=lambda x: x[1], reverse=True)[:n_show])\n",
    "print(sorted(wordcount_x.items(), key=lambda x: x[1])[:n_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kraino is a framework that helps in fast prototyping Visual Turing Test models\n",
    "from kraino.utils.input_output_space import build_vocabulary\n",
    "\n",
    "# This function takes wordcounts and returns word2index - mapping from words into indices, \n",
    "# and index2word - mapping from indices to words.\n",
    "word2index_x, index2word_x = build_vocabulary(\n",
    "    this_wordcount=wordcount_x,\n",
    "    truncate_to_most_frequent=0)\n",
    "word2index_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we are using a few special symbols that don't occur in the training dataset.\n",
    "Most important are $<pad>$ and $<unk>$. We will use the former to pad sequences in order to have the same \n",
    "number of temporal elements; we will use the latter for words (at test time) that don't exist in training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with vocabulary, we can build one-hot representation of the training data. However, this is not neccessary and maybe even wasteful. Our one-hot representation of the input text doesn't explicitely build long vectors, but instead it operates on indices. The example above would be encoded as [0,1,4,2,7,3]. \n",
    "```\n",
    "Can you prove the equivalence in the claim?\n",
    "```\n",
    "__claim__:\n",
    "\n",
    "Let $x$ be a binary vector with exactly one value $1$ at position $index$, that is $x[index]=1$. Then $$W[:,index] = Wx$$ where $W[:,b]$ denotes a vector built from a column $b$ of $W$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.utils.input_output_space import encode_questions_index\n",
    "one_hot_x = encode_questions_index(train_raw_x, word2index_x)\n",
    "print(train_raw_x[:3])\n",
    "print(one_hot_x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sequences have different elements. We will pad the sequences to have the same length $MAXLEN$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use another framework that is useful to build deep learning models - Keras\n",
    "from keras.preprocessing import sequence\n",
    "MAXLEN=30\n",
    "train_x = sequence.pad_sequences(one_hot_x, maxlen=MAXLEN)\n",
    "train_x[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the same with the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for simplicity, we consider only first answer words; that is, if answer is 'knife,fork' we encode only 'knife'\n",
    "MAX_ANSWER_TIME_STEPS=1\n",
    "\n",
    "from kraino.utils.input_output_space import encode_answers_one_hot\n",
    "train_raw_y = train_text_representation['y']\n",
    "wordcount_y = frequencies(' '.join(train_raw_y).split(' '))\n",
    "word2index_y, index2word_y = build_vocabulary(this_wordcount=wordcount_y)\n",
    "train_y, _ = encode_answers_one_hot(\n",
    "    train_raw_y, \n",
    "    word2index_y, \n",
    "    answer_words_delimiter=train_text_representation['answer_words_delimiter'],\n",
    "    is_only_first_answer_word=True,\n",
    "    max_answer_time_steps=MAX_ANSWER_TIME_STEPS)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also encode test questions. We need it later to see how well our models generalise to new question,answer,image triplets. Remember however that we should use vocabulary we generated from training samples.\n",
    "\n",
    "```\n",
    "Why should we use the training vocabulary to encode test questions?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text_representation = dp['text'](train_or_test='test')\n",
    "test_raw_x = test_text_representation['x']\n",
    "test_one_hot_x = encode_questions_index(test_raw_x, word2index_x)\n",
    "test_x = sequence.pad_sequences(test_one_hot_x, maxlen=MAXLEN)\n",
    "print_list(test_raw_x[:3])\n",
    "test_x[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With encoded question, answer pairs we finish the first section. But before delving into building and training new models, let's have a look at summary to see bigger picture.\n",
    "\n",
    "__Summary__\n",
    "\n",
    "We started from raw questions from the training set. Use them to build a vocabulary. Next, we encode questions into sequences of one-hot vectors based on the vocabulary. Finally, we use the same vocabulary to encode questions from test set, if a word is absent we use extra token $<unk>$ to encode this fact (we encode the $<unk>$ token, not the word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - overall picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We have textual features already built. Let's create some models that we will use for training and later for answering on questions about images.\n",
    "\n",
    "As you may already know, we train models by weights updates. Let $x$ and $y$ be training samples (input, output), and $\\ell(x,y)$ is an objective function. The formula for weights updates is:\n",
    "$$w := w - \\alpha \\nabla \\ell(x,y; w)$$\n",
    "with $\\alpha$ that we call the learning rate, and $\\nabla$ is  a gradient wrt. weights $w$. This is a hyper-parameter that must be set in advance. The rule shown above is called SGD update, but other its variants are also possible. In fact, we will use a variant called [ADAM](http://arxiv.org/pdf/1412.6980v8.pdf).\n",
    "\n",
    "We cast the question answering problem into a classification framework, so that we classify input $x$ into some class that represents an answer word. Therefore, we use, popular in classification, logistic regression as the objective:\n",
    "$$\\ell(x,y;w):=\\sum_{y' \\in \\mathcal{C}} \\mathbb{1}\\{y'=y\\}\\log p(y'\\;|\\;x,w)$$\n",
    "where $\\mathcal{C}$ is a set of all classes, and for $p(y\\;|\\;x,w)$ we will use softmax: $e^{w^y\\phi(x)} / \\sum_{z}e^{w^z\\phi(x)}$. Here $\\phi(x)$ denotes an output of a model (more precisely, it's often a neural network's response to the input, just before softmax).\n",
    "\n",
    "The training can be formalised (and automatised) so that you need to execute a procedure that looks something like that:\n",
    "```python\n",
    "training(gradient_of_the_model, optimizer='Adam')\n",
    "```\n",
    "\n",
    "__Summary__\n",
    "Given a model, and an optimization procedure (SGD, Adam, etc.) all we need is to compute gradient of the model $\\nabla \\ell(x,y;w)$ wrt. to its parameters $w$, and next plug it to the optimisation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since computing gradients $\\nabla \\ell(x,y;w)$ may become quickly tedious, especially for more complex models, we search for tools that could automitise it as well. Imagine that you build some model $M$ and you get its gradient $\\nabla M$ by just executing the tool\n",
    "```python\n",
    "nabla_M = compute_gradient_symbolically(M,x,y)\n",
    "```\n",
    "This would definitely speed up prototyping.\n",
    "\n",
    "[Theano](http://deeplearning.net/software/theano/) is such a tool that is specifically tailored to work with deep learning models. For broader understanding Theano, you can check [this nice tutorial](http://deeplearning.net/tutorial/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programmming example shown cell below defines ReLU, a popular activation function, as well as shows its derivative using Theano.\n",
    "However, with this example, we only scratch the surface.\n",
    "\n",
    "Assume ReLU is defined as follows $ReLU(x) = \\max(x,0)$.\n",
    "```\n",
    "What is the gradient of ReLU? Consider two cases. \n",
    "```\n",
    "Btw. ReLU is a nondifferentiable function, so technically we are computing its [subgradient](https://en.wikipedia.org/wiki/Subderivative) - it is still fine for Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# Theano is using symbolic calculations, so we need to first create symbolic variables\n",
    "theano_x = T.scalar()\n",
    "# we define a relationship between a symbolic input and a symbolic output\n",
    "theano_y = T.maximum(0,theano_x)\n",
    "# now it's time for a symbolic gradient wrt. to symbolic variable x\n",
    "theano_nabla_y = T.grad(theano_y, theano_x)\n",
    "\n",
    "# we can see that both variables are symbolic, they don't have any numerical values\n",
    "print(theano_x)\n",
    "print(theano_y)\n",
    "print(theano_nabla_y)\n",
    "\n",
    "# theano.function compiles the symbolic representation of the network\n",
    "theano_f_x = theano.function([theano_x], theano_y)\n",
    "print(theano_f_x(3))\n",
    "print(theano_f_x(-3))\n",
    "# and now for gradients\n",
    "\n",
    "nabla_f_x = theano.function([theano_x], theano_nabla_y)\n",
    "print(nabla_f_x(3))\n",
    "print(nabla_f_x(-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary__\n",
    "\n",
    "To compute gradient symbolically, we can use [Theano](http://deeplearning.net/software/theano/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Keras](http://keras.io) builds on top of Theano, and significantly simplifies creating new models as well as training such models, effectively speeding up the prototyping even further. Keras also abstracts away from some technical burden such as symbolic variable creation. Metaphorically, while Theano can be seen as a deep learning equivalent of assembler, Keras is more like Java :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we sample from noisy x^2 function\n",
    "from numpy import asarray\n",
    "from numpy import random\n",
    "def myfun(x):\n",
    "    return x*x\n",
    "\n",
    "NUM_SAMPLES = 10000\n",
    "HIGH_VALUE=10\n",
    "keras_x = asarray(random.randint(low=0, high=HIGH_VALUE, size=NUM_SAMPLES))\n",
    "keras_noise = random.normal(loc=0.0, scale=0.1, size=NUM_SAMPLES)\n",
    "keras_noise = asarray([max(x,0) for x in keras_noise])\n",
    "keras_y = asarray([myfun(x) + n for x,n in zip(keras_x, keras_noise)])\n",
    "# print('X:')\n",
    "# print(keras_x)\n",
    "# print('Noise')\n",
    "# print(keras_noise)\n",
    "# print('Noisy X^2:')\n",
    "# print(keras_y)\n",
    "\n",
    "keras_x = keras_x.reshape(keras_x.shape[0],1)\n",
    "keras_y = keras_y.reshape(keras_y.shape[0],1)\n",
    "\n",
    "# import keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# build a regression network\n",
    "KERAS_NUM_HIDDEN = 150\n",
    "KERAS_NUM_HIDDEN_SECOND = 150\n",
    "KERAS_NUM_HIDDEN_THIRD = 150\n",
    "KERAS_DROPOUT_FRACTION = 0.5\n",
    "m = Sequential()\n",
    "m.add(Dense(KERAS_NUM_HIDDEN, input_dim=1))\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(KERAS_DROPOUT_FRACTION))\n",
    "#TODO: add one more layer\n",
    "# m.add(Dense(KERAS_NUM_HIDDEN_SECOND))\n",
    "# m.add(Activation('relu'))\n",
    "# m.add(Dropout(KERAS_DROPOUT_FRACTION))\n",
    "#TODO: add one more layer\n",
    "# m.add(Dense(KERAS_NUM_HIDDEN_THIRD))\n",
    "# m.add(Activation('relu'))\n",
    "# m.add(Dropout(KERAS_DROPOUT_FRACTION))\n",
    "m.add(Dense(1))\n",
    "\n",
    "# compile and fit\n",
    "m.compile(loss='mse', optimizer='adam')\n",
    "m.fit(keras_x, keras_y, nb_epoch=100, batch_size=250)\n",
    "\n",
    "keras_x_predict = asarray([1,3,6,12,HIGH_VALUE+10])\n",
    "keras_x_predict = keras_x_predict.reshape(keras_x_predict.shape[0],1)\n",
    "keras_predictions = m.predict(keras_x_predict)\n",
    "print(\"{0:>10}{1:>10}{2:>10}\".format('X', 'Y', 'GT'))\n",
    "for x,y in zip(keras_x_predict, keras_predictions):\n",
    "    print(\"{0:>10}{1:>10.2f}{2:>10}\".format(x[0], y[0], myfun(x[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the example above.\n",
    "\n",
    " * What happens if you add one more layer? Or two more layers?\n",
    " * What happens if you change hidden size?\n",
    " * What happens if you use more/less samples?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the purpose of Visual Turing Test, and this tutorial, we have compiled a light framework that builds on top of Keras, and simplify building and training question answering machines. With the tradition of using fancy Greek names, we call it Kraino. Note that some parts of the Kraino, such as data provider, you have already seen. \n",
    "\n",
    "In the following, we will go through BOW and LSTM approaches to answer questions about images, but, surprisingly, without the images. It turns out that a substantial fraction of questions can be answered without an access to an image, but rather by resorting to common sense (or statistics of the dataset). For instance, what can be placed at the table? How many eyes this human have?. Answers like 'chair' and '2' are quite likely to be good answers.\n",
    "\n",
    "Please make sure that all the cells from [Datasets](#Datasets) and [Textual Features](#Textual-Features) have been executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates BOW (Bag Of Words) approach. As we have already seen in [Textual Features](#Textual-Features), we first encode the input sentence into one-hot vector representations. Such (very) sparse representation is next embedded into a denser space by a matrix $W_e$. Next, the denser representations are summed up and classified via 'Softmax'. Notice that, if $W_e$ were an identity matrix, we would obtain a histogram of the word's occurrences. \n",
    "\n",
    "\n",
    "```\n",
    "What is your biggest complain about such BOW representation? What happens if instead of \n",
    "'What is behind the table' we would have 'is What the behind table'? How does the BOW representation change? \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bow](fig/BOW_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model definition\n",
    "\n",
    "# First we define a model using keras/kraino\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import TimeDistributedMerge\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from kraino.core.model_zoo import AbstractSequentialModel\n",
    "from kraino.core.model_zoo import AbstractSingleAnswer\n",
    "from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer\n",
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.keras_extensions import DropMask\n",
    "from kraino.core.keras_extensions import LambdaWithMask\n",
    "from kraino.core.keras_extensions import time_distributed_masked_ave\n",
    "\n",
    "# This model inherits from AbstractSingleAnswer, and so it produces single answer words\n",
    "# To use multiple answer words, you need to inherit from AbstractSequentialMultiplewordAnswer\n",
    "class BlindBOW(AbstractSequentialModel, AbstractSingleAnswer):\n",
    "    \"\"\"\n",
    "    BOW Language only model that produces single word answers.\n",
    "    \"\"\"\n",
    "    def create(self):\n",
    "        self.add(Embedding(\n",
    "                self._config.input_dim, \n",
    "                self._config.textual_embedding_dim, \n",
    "                mask_zero=True))\n",
    "        self.add(LambdaWithMask(time_distributed_masked_ave, output_shape=[self.output_shape[2]]))\n",
    "        self.add(DropMask())\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(self._config.output_dim))\n",
    "        self.add(Activation('softmax'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_config = Config(\n",
    "    textual_embedding_dim=500,\n",
    "    input_dim=len(word2index_x.keys()),\n",
    "    output_dim=len(word2index_y.keys()))\n",
    "model = BlindBOW(model_config)\n",
    "model.create()\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')\n",
    "text_bow_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_bow_model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although BOW is working pretty well, there is still something very disturbing about this approach.\n",
    "Consider the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we swap 'chair' with 'telephone' in the question, we would get a different meaning, wouldn't we? Recurrent Neural Networks (RNNs) have been developed to mitigate this issue by directly processing the time series. As the figure below illustrates, the (temporarily) first word's embedding is given to an RNN unit. The RNN unit next 'processes' such embedding and outputs to the second RNN unit. This unit takes both the output of the first RNN unit and the 2nd word's embedding as inputs, and outputs some algebraic combination of both inputs. And so on. The last recurrent unit builds the representation of the whole sequence. Its output is next given to Softmax for the classification. One of the challenged that such approaches have to deal with are keeping long-term dependencies. Roughly speaking, as new inputs are coming it's getting easier to 'forget' information from the beginning. [LSTM](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) and [GRU](http://www.aclweb.org/anthology/W14-4012) are two particularly successful Recurrent Neural Networks that can preserve such longer dependencies to [some degree](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "__Note__: If the code below is not compiling, please restart the notebook, and run only [Datasets](#Datasets) and [Textual Features](#Textual-Features). In particular, don't run BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](fig/LSTM_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model definition\n",
    "\n",
    "# First we define a model using keras/kraino\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from kraino.core.model_zoo import AbstractSequentialModel\n",
    "from kraino.core.model_zoo import AbstractSingleAnswer\n",
    "from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer\n",
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.keras_extensions import DropMask\n",
    "from kraino.core.keras_extensions import LambdaWithMask\n",
    "from kraino.core.keras_extensions import time_distributed_masked_ave\n",
    "\n",
    "# This model inherits from AbstractSingleAnswer, and so it produces single answer words\n",
    "# To use multiple answer words, you need to inherit from AbstractSequentialMultiplewordAnswer\n",
    "class BlindRNN(AbstractSequentialModel, AbstractSingleAnswer):\n",
    "    \"\"\"\n",
    "    RNN Language only model that produces single word answers.\n",
    "    \"\"\"\n",
    "    def create(self):\n",
    "        self.add(Embedding(\n",
    "                self._config.input_dim, \n",
    "                self._config.textual_embedding_dim, \n",
    "                mask_zero=True))\n",
    "        #TODO: Replace averaging with RNN (you can choose between LSTM and GRU)\n",
    "#         self.add(LambdaWithMask(time_distributed_masked_ave, output_shape=[self.output_shape[2]]))\n",
    "        self.add(GRU(self._config.hidden_state_dim, \n",
    "                      return_sequences=False))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(self._config.output_dim))\n",
    "        self.add(Activation('softmax'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_config = Config(\n",
    "    textual_embedding_dim=500,\n",
    "    hidden_state_dim=500,\n",
    "    input_dim=len(word2index_x.keys()),\n",
    "    output_dim=len(word2index_y.keys()))\n",
    "model = BlindRNN(model_config)\n",
    "model.create()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')\n",
    "text_rnn_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_rnn_model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this Tutorial, you are free to experiment with two examples above.\n",
    "* You can change the size of embedding.\n",
    "* You can change the number of hidden state of RNN.\n",
    "* You can change number of epochs to train.\n",
    "* You can experiment with different batch sizes.\n",
    "* You can modify the models (many RNN layers, deeper classifiers). Use [Keras](http://keras.io) documentation if you are in needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary__\n",
    "\n",
    "RNN models, as opposite to BOW, consider order of the words in the question. Moreover, apparently, a substantial number of questions can be answered without any access to image. This can be explained as models learn some specific dataset biases, some of them can be interpreted as common sense knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, please run the cell below to set up a link to the NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env NLTK_DATA=/home/ubuntu/data/visual_turing_test/nltk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to monitor progress on any task, we need to find ways to evaluate the task. Otherwise, we wouldn't know how to compare two architectures, or even worse, we wouldn't even know what our goal is. Moreover, we should also aim at automatic evaluation measures, otherwise reproducibility is questionable, and the costs are high (speed and money; just imagine that you want to evaluate 100 different architectures of yours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, it's difficult to automatically evaluate holistic tasks such as question answering about images, because of, in just one word, ambiguities. We have ambiguities in naming objects, sometimes due to synonyms, but sometimes due to fuzziness. For instance is 'chair' == 'armchair' or 'chair' != 'armchair' or something in between? Such semantic boundaries become even more fuzzy when we increase the number of categories. We could easily find a mutually exclusive set of 10 different categories, but what if there are 1000 categories, or 10000 categories? Arguably, we cannot think in terms of equivalence class anymore, but rather in terms of similarities. That is 'chair' is semantically more similar to 'armchair', than to 'horse'. This simple example shows the main drawback of a traditional binary evaluation measure Accuracy, which scores 1 if the names are the same and 0 otherwise. So that Acc('chair', 'armchair') == Acc('chair', 'horse'). We use WUPS to handle such ambiguities.\n",
    "\n",
    "We call these ambiguities, word-level ambiguities, but there are other ambiguities that are arguably more difficult to handle. For instance, the same question can be phrased in multiple other ways. The language of spatial relations is also ambiguous (you may be surprised that what you think is on the left, for others may be on the right). Language tends to be also rather vague - we sometimes skip details and resort to common sense. Some such ambiguities are rooted in a culture. A couple of such question-level ambiguities, we handle with Consensus Measure.\n",
    "\n",
    "From an another side, arguably, it's easier to evaluate architectures on DAQUAR than on the Image Captioning tasks. The former restricts the output space to $N$ categories, while it still requires holistic (visual and linguistic) comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu-Palmer Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an ontology a Wu-Palmer Similarity between two words (or broader concepts) is a soft measure defined as\n",
    "$$WuP(a,b) := \\frac{lca(a,b)}{depth(a) + depth(b)}$$\n",
    "where $lca(a,b)$ is the least common ancestor of $a$ and $b$, and $depth(a)$ is depth of $a$ in the ontology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![small taxonomy](fig/small_taxonomy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "What is WuP(Dog, Horse) and WuP(Dog, Dalmatian) according to the ontology above? \n",
    "Can you also calculate Acc(Dog, Horse) and Acc(Dog, Dalmatian)?\n",
    "What are your conclusions?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WUPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wu-Palmer Similarity depends on a ontology. One popular, large ontology is [WordNet](https://wordnet.princeton.edu). Although Wu-Palmer Similarity may work on shallow ontologies, we are rather interested in ontologies with hundreds or even thousands categories. In indoor scenerio, it turns out that many indoor things share similar levels in the taxomy, and hence Wu-Palmer Similarities are very small between each other.\n",
    "\n",
    "The code below exemplifies the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "armchair_synset = wn.synset('armchair.n.01')\n",
    "chair_synset = wn.synset('chair.n.01')\n",
    "wardrobe_synset = wn.synset('wardrobe.n.01')\n",
    "\n",
    "print(armchair_synset.wup_similarity(armchair_synset))\n",
    "print(armchair_synset.wup_similarity(chair_synset))\n",
    "print(armchair_synset.wup_similarity(wardrobe_synset))\n",
    "wn.synset('chair.n.01').wup_similarity(wn.synset('person.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code we see that 'armchair' and 'wardrobe' are surprisingly close to each other. It is because, for large ontologies like [WordNet](https://wordnet.princeton.edu), all the indoor things are essentially 'indoor things'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue has motivated us to define thresholded Wu-Palmer Similarity Score, defined as follows\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "WuP(a,b) & \\text{if}\\; WuP(a,b) \\ge \\tau \\\\\n",
    "0.1 \\cdot WuP(a,b) & \\text{otherwise}\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\tau$ is a hand-chosen threshold. Empirically, we found that $\\tau=0.9$ works fine on DAQUAR.\n",
    "\n",
    "Moreover, since DAQUAR has answers as set answer words, so that 'knife,fork' == 'fork,knife', we have extended the above measure to work with sets. We call it Wu-Palmer Set score, or shortly WUPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed exposition of WUPS is beyond this tutorial, but a curious reader is encoraged to read the 'Performance Measure' paragraph in [our paper](http://arxiv.org/pdf/1410.0210v4.pdf). Note that the measure in [the paper](http://arxiv.org/pdf/1410.0210v4.pdf) is defined broader, and essentially it abstracts from any particular similarities such as Wu-Palmer Similarity. WUPS at 0.9 is WUPS with threshold $\\tau=0.9$.\n",
    "\n",
    "Although the WUPS is conceptually as we described here, technically, it's slightly different as it also needs to deal with synsets. Thus it's recommended to download the script from [here](http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py), or re-implement with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we won't cover the consensus measure.\n",
    "The curious reader is encouraged to read the 'Human Consensus' in the [Ask Your Neurons paper](https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few caveats with WUPS, especially useful if you want to use the measure to your own dataset.\n",
    "\n",
    "__Lack of coverage__\n",
    "Since WUPS is based on an ontology, not always it recognises words. For instance 'garbage bin' is missing, but 'garbage can' is perfectly fine. You can check it by yourself, either with the source code provided above, or by using [this online script](http://wordnetweb.princeton.edu/perl/webwn).\n",
    "\n",
    "__Synsets__\n",
    "If you execute \n",
    "```python\n",
    "wn.synsets('chair')\n",
    "```\n",
    "you will notice a list with many elements, these elements are [semantically equivalent](https://en.wikipedia.org/wiki/Synonym_ring). You can check their definitions, for instance\n",
    "```python\n",
    "wn.synset('chair.n.03').definition()\n",
    "```\n",
    "indicates a person. Indeed, the following has quite high value\n",
    "```python\n",
    "wn.synset('chair.n.03').wup_similarity(wn.synset('person.n.01'))\n",
    "```\n",
    "but this one has a more preffered low value\n",
    "```python\n",
    "wn.synset('chair.n.01').wup_similarity(wn.synset('person.n.01'))\n",
    "```\n",
    "How to deal with it? In DAQUAR we take an optimistic perspective and always consider the highest similarity score. This works with WUPS 0.9 and a restricted indoor domain with a vocabulary only from the trainin set. This may not be true in other domains though.\n",
    "\n",
    "__Ontology__\n",
    "Since WUPS is based on an ontology, specifically on WordNet, it may give different scores on different ontologies, or even on different versions of the same ontology.\n",
    "\n",
    "__Threshold__\n",
    "A good threshold $\\tau$ is dataset dependent. In our case $\\tau = 0.9$ seems to work well, while $\\tau = 0.0$ is too forgivable and is rather reported due to the 'historical' reasons. However, following our papers, you should still consider to report plain set-based accuracy scores (so that Acc('knife,'fork','fork,knife')==1; it can be computed with WUPS -1 using [our script](http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py)) as this metric is widely recognised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WUPS is an evaluation measure that works with sets and word-level ambiguities. Arguably, WUPS at 0.9 is the most practical measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions - BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more and more iterations we can increase training accuracy, however our goal is to see how well the models generalise. For that, we take a test, previously unknown, set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text_representation = dp['text'](train_or_test='test')\n",
    "test_raw_x = test_text_representation['x']\n",
    "test_one_hot_x = encode_questions_index(test_raw_x, word2index_x)\n",
    "test_x = sequence.pad_sequences(test_one_hot_x, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given encoded test questions, we use the maximum likelihood principle to withdraw answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# predict the probabilities for every word\n",
    "predictions_scores = text_bow_model.predict([test_x])\n",
    "print(predictions_scores.shape)\n",
    "# follow the maximum likelihood principle, and get the best indices to vocabulary\n",
    "predictions_best = argmax(predictions_scores, axis=-1)\n",
    "print(predictions_best.shape)\n",
    "# decode the predicted indices into word answers\n",
    "predictions_answers = [index2word_y[x] for x in predictions_best]\n",
    "print(len(predictions_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the answers using [WUPS scores](#Evaluation-Measures). For this tutorial, we care only about Accuracy, and WUPS at 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.utils import print_metrics\n",
    "test_raw_y = test_text_representation['y']\n",
    "_ = print_metrics.select['wups'](\n",
    "        gt_list=test_raw_y,\n",
    "        pred_list=predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "test_image_name_list = test_text_representation['img_name']\n",
    "indices_to_see = random.randint(low=0, high=len(test_image_name_list), size=5)\n",
    "for index_now in indices_to_see:\n",
    "    print(test_raw_x[index_now], predictions_answers[index_now])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Do you agree with the answers given above? What are your guesses?\n",
    "Of course,neither you nor the model have seen any images so far.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if you actually see the images? \n",
    "```\n",
    "Execute the code below.\n",
    "Do your answers change after seeing the images?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import axis\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "for index_now in indices_to_see:\n",
    "    image_name_now = test_image_name_list[index_now]\n",
    "    pil_im = Image.open('data/daquar/images/{0}.png'.format(image_name_now), 'r')\n",
    "    fig = figure()\n",
    "    fig.text(.2,.05,test_raw_x[index_now], fontsize=14)\n",
    "    axis('off')\n",
    "    imshow(np.asarray(pil_im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also see the ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('question, prediction, ground truth answer')\n",
    "for index_now in indices_to_see:\n",
    "    print(test_raw_x[index_now], predictions_answers[index_now], test_raw_y[index_now])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we have randomly taken questions, so for different executations we may get different answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious how predictions with blind RNN went? \n",
    "\n",
    "This time, we will use the help of Kraino, to make the predictions shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.core.model_zoo import word_generator\n",
    "# we first need to add word_generator to _config (we could have done this before, in the Config constructor)\n",
    "# we use maximum likelihood as a word generator\n",
    "text_rnn_model._config.word_generator = word_generator['max_likelihood']\n",
    "predictions_answers = text_rnn_model.decode_predictions(\n",
    "    X=test_x,\n",
    "    temperature=None,\n",
    "    index2word=index2word_y,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = print_metrics.select['wups'](\n",
    "        gt_list=test_raw_y,\n",
    "        pred_list=predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Visualise question, predicted answers, ground truth answers as before.\n",
    "Check also images.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go very far using only textual features. Hence, it's now time to consider its visual counterpart.\n",
    "\n",
    "As shown in the figure below, a quite common procedure works as follows:\n",
    "* Use a CNN already pre-trained on some large-scale classification task, most often it is [ImageNet](http://image-net.org) with $1000$ for recognition.\n",
    "* 'Chop off' CNN after some layer. We will use responses of that layer as visual features.\n",
    "\n",
    "In this tutorial, we will use features extracted from the second last $4096$ dimensional layer of [VGG NET-19](http://arxiv.org/pdf/1409.1556.pdf). We have already extracted features in advance using [Caffe](http://caffe.berkeleyvision.org) - another excellent framework for deep learning, particularly good for CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![features extractor](fig/features_extractor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below in order to get visual features aligned with textual featurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this contains a list of the image names of our interest; \n",
    "# it also makes sure that visual and textual features are aligned correspondingly\n",
    "train_image_names = train_text_representation['img_name']\n",
    "# the name for visual features that we use\n",
    "# CNN_NAME='vgg_net'\n",
    "# CNN_NAME='googlenet'\n",
    "CNN_NAME='fb_resnet'\n",
    "# the layer in CNN that is used to extract features\n",
    "# PERCEPTION_LAYER='fc7'\n",
    "# PERCEPTION_LAYER='pool5-7x7_s1'\n",
    "# PERCEPTION_LAYER='res5c-152'\n",
    "PERCEPTION_LAYER='l2_res5c-152' # l2 prefix since there are l2-normalized visual features\n",
    "\n",
    "train_visual_features = dp['perception'](\n",
    "    train_or_test='train',\n",
    "    names_list=train_image_names,\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=CNN_NAME,\n",
    "    layer=PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "train_visual_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision+Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are talking about answering on questions about images, we likely need images too :)\n",
    "\n",
    "Take a look at the figure below one more time. How far can you go by blind guesses? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![challenges](fig/challenges.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's creat an input as a pair of textual and visual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input = [train_x, train_visual_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW + Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Language Only model, we start from a simpler BOW model that we will combine with [visual features](fig/#Visual Features). Here, we will explore two ways of combining both modalities (circle with 'C' in the figure below): concatenation, and piece-wise multiplication. We will use CNN features extracted from the image, but for the sake of simplicity we won't backprop to fine tune the visual representation (dot line symbolizes the barrier that blocks back-prop in the figure below). Although in our [Ask Your Neurons](http://arxiv.org/abs/1505.01121) fine-tuning the last layer was actually important, benefits of end-to-end training on DAQUAR or larger [VQA](http://visualqa.org) datasets remain an open question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BOW_vision](fig/BOW_vision_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model definition\n",
    "\n",
    "# First we define a model using keras/kraino\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Layer\n",
    "from keras.layers.core import Merge\n",
    "from keras.layers.core import TimeDistributedMerge\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from kraino.core.model_zoo import AbstractSequentialModel\n",
    "from kraino.core.model_zoo import AbstractSingleAnswer\n",
    "from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer\n",
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.keras_extensions import DropMask\n",
    "from kraino.core.keras_extensions import LambdaWithMask\n",
    "from kraino.core.keras_extensions import time_distributed_masked_ave\n",
    "\n",
    "# This model inherits from AbstractSingleAnswer, and so it produces single answer words\n",
    "# To use multiple answer words, you need to inherit from AbstractSequentialMultiplewordAnswer\n",
    "class VisionLanguageBOW(AbstractSequentialModel, AbstractSingleAnswer):\n",
    "    \"\"\"\n",
    "    BOW Language only model that produces single word answers.\n",
    "    \"\"\"\n",
    "    def create(self):\n",
    "        language_model = Sequential()\n",
    "        language_model.add(Embedding(\n",
    "                self._config.input_dim, \n",
    "                self._config.textual_embedding_dim, \n",
    "                mask_zero=True))\n",
    "        language_model.add(LambdaWithMask(\n",
    "                time_distributed_masked_ave, \n",
    "                output_shape=[language_model.output_shape[2]]))\n",
    "        language_model.add(DropMask())\n",
    "        visual_model = Sequential()\n",
    "        if self._config.visual_embedding_dim > 0:\n",
    "            visual_model.add(Dense(\n",
    "                    self._config.visual_embedding_dim,\n",
    "                    input_shape=(self._config.visual_dim,)))\n",
    "        else:\n",
    "            visual_model.add(Layer(input_shape=(self._config.visual_dim,)))\n",
    "        self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(self._config.output_dim))\n",
    "        self.add(Activation('softmax'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensionality of embeddings\n",
    "EMBEDDING_DIM = 500\n",
    "# kind of multimodal fusion (ave, concat, mul, sum)\n",
    "MULTIMODAL_MERGE_MODE = 'concat'\n",
    "\n",
    "model_config = Config(\n",
    "    textual_embedding_dim=EMBEDDING_DIM,\n",
    "    visual_embedding_dim=0,\n",
    "    multimodal_merge_mode=MULTIMODAL_MERGE_MODE,\n",
    "    input_dim=len(word2index_x.keys()),\n",
    "    output_dim=len(word2index_y.keys()),\n",
    "    visual_dim=train_visual_features.shape[1])\n",
    "model = VisionLanguageBOW(model_config)\n",
    "model.create()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "model.fit(\n",
    "    train_input, \n",
    "    train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, if we use a piece-wise multiplication to merge both modalities together, we will get better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#== Model definition\n",
    "\n",
    "# First we define a model using keras/kraino\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Layer\n",
    "from keras.layers.core import Merge\n",
    "from keras.layers.core import TimeDistributedMerge\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from kraino.core.model_zoo import AbstractSequentialModel\n",
    "from kraino.core.model_zoo import AbstractSingleAnswer\n",
    "from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer\n",
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.keras_extensions import DropMask\n",
    "from kraino.core.keras_extensions import LambdaWithMask\n",
    "from kraino.core.keras_extensions import time_distributed_masked_ave\n",
    "\n",
    "# This model inherits from AbstractSingleAnswer, and so it produces single answer words\n",
    "# To use multiple answer words, you need to inherit from AbstractSequentialMultiplewordAnswer\n",
    "class VisionLanguageBOW(AbstractSequentialModel, AbstractSingleAnswer):\n",
    "    \"\"\"\n",
    "    BOW Language only model that produces single word answers.\n",
    "    \"\"\"\n",
    "    def create(self):\n",
    "        language_model = Sequential()\n",
    "        language_model.add(Embedding(\n",
    "                self._config.input_dim, \n",
    "                self._config.textual_embedding_dim, \n",
    "                mask_zero=True))\n",
    "        language_model.add(LambdaWithMask(\n",
    "                time_distributed_masked_ave, \n",
    "                output_shape=[language_model.output_shape[2]]))\n",
    "        language_model.add(DropMask())\n",
    "        visual_model = Sequential()\n",
    "        if self._config.visual_embedding_dim > 0:\n",
    "            visual_model.add(Dense(\n",
    "                    self._config.visual_embedding_dim,\n",
    "                    input_shape=(self._config.visual_dim,)))\n",
    "        else:\n",
    "            visual_model.add(Layer(input_shape=(self._config.visual_dim,)))\n",
    "        self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(self._config.output_dim))\n",
    "        self.add(Activation('softmax'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensionality of embeddings\n",
    "EMBEDDING_DIM = 500\n",
    "# kind of multimodal fusion (ave, concat, mul, sum)\n",
    "MULTIMODAL_MERGE_MODE = 'mul'\n",
    "\n",
    "model_config = Config(\n",
    "    textual_embedding_dim=EMBEDDING_DIM,\n",
    "    visual_embedding_dim=EMBEDDING_DIM,\n",
    "    multimodal_merge_mode=MULTIMODAL_MERGE_MODE,\n",
    "    input_dim=len(word2index_x.keys()),\n",
    "    output_dim=len(word2index_y.keys()),\n",
    "    visual_dim=train_visual_features.shape[1])\n",
    "model = VisionLanguageBOW(model_config)\n",
    "model.create()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')\n",
    "text_image_bow_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "If we merge language and visual features with 'mul', do we need to set both embeddings to have the same number  of dimensions (textual_embedding_dim == visual_embedding_dim)?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_image_bow_model.fit(\n",
    "    train_input, \n",
    "    train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat the BOW experiments but with RNN.\n",
    "\n",
    "![LSTM_vision](fig/LSTM_vision_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model definition\n",
    "\n",
    "# First we define a model using keras/kraino\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Layer\n",
    "from keras.layers.core import Merge\n",
    "from keras.layers.core import TimeDistributedMerge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from kraino.core.model_zoo import AbstractSequentialModel\n",
    "from kraino.core.model_zoo import AbstractSingleAnswer\n",
    "from kraino.core.model_zoo import AbstractSequentialMultiplewordAnswer\n",
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.keras_extensions import DropMask\n",
    "from kraino.core.keras_extensions import LambdaWithMask\n",
    "from kraino.core.keras_extensions import time_distributed_masked_ave\n",
    "\n",
    "# This model inherits from AbstractSingleAnswer, and so it produces single answer words\n",
    "# To use multiple answer words, you need to inherit from AbstractSequentialMultiplewordAnswer\n",
    "class VisionLanguageLSTM(AbstractSequentialModel, AbstractSingleAnswer):\n",
    "    \"\"\"\n",
    "    BOW Language only model that produces single word answers.\n",
    "    \"\"\"\n",
    "    def create(self):\n",
    "        language_model = Sequential()\n",
    "        language_model.add(Embedding(\n",
    "                self._config.input_dim, \n",
    "                self._config.textual_embedding_dim, \n",
    "                mask_zero=True))\n",
    "        #TODO: Replace averaging with RNN (you can choose between LSTM and GRU)\n",
    "#         language_model.add(LambdaWithMask(time_distributed_masked_ave, output_shape=[self.output_shape[2]]))\n",
    "        language_model.add(LSTM(self._config.hidden_state_dim, \n",
    "                      return_sequences=False))\n",
    "\n",
    "        visual_model = Sequential()\n",
    "        if self._config.visual_embedding_dim > 0:\n",
    "            visual_model.add(Dense(\n",
    "                    self._config.visual_embedding_dim,\n",
    "                    input_shape=(self._config.visual_dim,)))\n",
    "        else:\n",
    "            visual_model.add(Layer(input_shape=(self._config.visual_dim,)))\n",
    "        self.add(Merge([language_model, visual_model], mode=self._config.multimodal_merge_mode))\n",
    "        self.add(Dropout(0.5))\n",
    "        self.add(Dense(self._config.output_dim))\n",
    "        self.add(Activation('softmax'))\n",
    "        \n",
    "        \n",
    "# dimensionality of embeddings\n",
    "EMBEDDING_DIM = 500\n",
    "# kind of multimodal fusion (ave, concat, mul, sum)\n",
    "MULTIMODAL_MERGE_MODE = 'sum'\n",
    "\n",
    "model_config = Config(\n",
    "    textual_embedding_dim=EMBEDDING_DIM,\n",
    "    visual_embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_state_dim=EMBEDDING_DIM,\n",
    "    multimodal_merge_mode=MULTIMODAL_MERGE_MODE,\n",
    "    input_dim=len(word2index_x.keys()),\n",
    "    output_dim=len(word2index_y.keys()),\n",
    "    visual_dim=train_visual_features.shape[1])\n",
    "model = VisionLanguageLSTM(model_config)\n",
    "model.create()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')\n",
    "text_image_rnn_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, again, let's train the model (if the following cell crashes, please move to the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_image_rnn_model.fit(\n",
    "    train_input, \n",
    "    train_y,\n",
    "    batch_size=5500,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops, apparently we run out of memory in our GPUs. Note, how large our batches are! \n",
    "Let's make them much smaller (argument batch_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_image_rnn_model.fit(\n",
    "    train_input, \n",
    "    train_y,\n",
    "    batch_size=1,\n",
    "    nb_epoch=1,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Please, stop it! Batch size 1 is not good neither. Training is very slow.\n",
    "Let's use standard batch size 512. Please re-run the cell with the model definition, and next run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "text_image_rnn_model.fit(\n",
    "    train_input, \n",
    "    train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=40,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Can you explain both issues regarding the batch size? Why training is impossible in the first case, and very tedious in the second case?\n",
    "\n",
    "When do you get the best performance, with multiplication, concatenation, or summation?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary__\n",
    "\n",
    "As previously, using RNN makes the sequence processing order-aware. This time, however, we combine two modalities so that the whole model 'sees' the image. Finally, it's important how both modalities are combined, we have found that piece-wise multiplication outperforms traditional concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Predictions with Vision+Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_image_names = test_text_representation['img_name']\n",
    "test_visual_features = dp['perception'](\n",
    "    train_or_test='test',\n",
    "    names_list=test_image_names,\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=CNN_NAME,\n",
    "    layer=PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "test_visual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = [test_x, test_visual_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions (Bow with Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the Vision+Language architectures as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.core.model_zoo import word_generator\n",
    "# we first need to add word_generator to _config (we could have done this before, in the Config constructor)\n",
    "# we use maximum likelihood as a word generator\n",
    "text_image_bow_model._config.word_generator = word_generator['max_likelihood']\n",
    "predictions_answers = text_image_bow_model.decode_predictions(\n",
    "    X=test_input,\n",
    "    temperature=None,\n",
    "    index2word=index2word_y,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = print_metrics.select['wups'](\n",
    "        gt_list=test_raw_y,\n",
    "        pred_list=predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions (RNN with Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.core.model_zoo import word_generator\n",
    "# we first need to add word_generator to _config (we could have done this before, in the Config constructor)\n",
    "# we use maximum likelihood as a word generator\n",
    "text_image_rnn_model._config.word_generator = word_generator['max_likelihood']\n",
    "predictions_answers = text_image_rnn_model.decode_predictions(\n",
    "    X=test_input,\n",
    "    temperature=None,\n",
    "    index2word=index2word_y,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = print_metrics.select['wups'](\n",
    "        gt_list=test_raw_y,\n",
    "        pred_list=predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that we have built so far can be transferred to other dataset.\n",
    "Let's consider recently introduced large-scale [VQA](visual question answering) built on top of [COCO](http://mscoco.org). In this section, we will train and evaluate VQA models. Since we are using all pieces introduced before, we will just quickly go into coding. For the sake of simplicity, we will use only BOW architectures, but you are free to experiment with RNN. Please also to pay attention to the comments.\n",
    "\n",
    "Since VQA hides the test data for the purpose of challenge, we will use the publically validation set to evaluate the architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA Language Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: Execute the following procedure (Shift+Enter)\n",
    "from kraino.utils import data_provider\n",
    "\n",
    "vqa_dp = data_provider.select['vqa-real_images-open_ended']\n",
    "# VQA has a few answers associated with one question. \n",
    "# We take the most frequently occuring answers (single_frequent).\n",
    "# Formal argument 'keep_top_qa_pairs' allows to filter out rare answers with the associated questions.\n",
    "# We use 0 as we want to keep all question answer pairs, but you can change into 1000 and see how the results differ\n",
    "vqa_train_text_representation = vqa_dp['text'](\n",
    "    train_or_test='train',\n",
    "    answer_mode='single_frequent',\n",
    "    keep_top_qa_pairs=1000)\n",
    "vqa_val_text_representation = vqa_dp['text'](\n",
    "    train_or_test='val',\n",
    "    answer_mode='single_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from toolz import frequencies\n",
    "vqa_train_raw_x = vqa_train_text_representation['x']\n",
    "vqa_train_raw_y = vqa_train_text_representation['y']\n",
    "vqa_val_raw_x = vqa_val_text_representation['x']\n",
    "vqa_val_raw_y = vqa_val_text_representation['y']\n",
    "# we start from building the frequencies table\n",
    "vqa_wordcount_x = frequencies(' '.join(vqa_train_raw_x).split(' '))\n",
    "# we can keep all answer words in the answer as a class\n",
    "# therefore we use an artificial split symbol '{' to not split the answer into words\n",
    "# you can see the difference if you replace '{' with ' ' and print vqa_wordcount_y\n",
    "vqa_wordcount_y = frequencies('{'.join(vqa_train_raw_y).split('{'))\n",
    "vqa_wordcount_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language-Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from kraino.utils.input_output_space import build_vocabulary\n",
    "from kraino.utils.input_output_space import encode_questions_index\n",
    "from kraino.utils.input_output_space import encode_answers_one_hot\n",
    "MAXLEN=30\n",
    "vqa_word2index_x, vqa_index2word_x = build_vocabulary(this_wordcount = vqa_wordcount_x)\n",
    "vqa_word2index_y, vqa_index2word_y = build_vocabulary(this_wordcount = vqa_wordcount_y)\n",
    "vqa_train_x = sequence.pad_sequences(encode_questions_index(vqa_train_raw_x, vqa_word2index_x), maxlen=MAXLEN)\n",
    "vqa_val_x = sequence.pad_sequences(encode_questions_index(vqa_val_raw_x, vqa_word2index_x), maxlen=MAXLEN)\n",
    "vqa_train_y, _ = encode_answers_one_hot(\n",
    "    vqa_train_raw_y, \n",
    "    vqa_word2index_y, \n",
    "    answer_words_delimiter=vqa_train_text_representation['answer_words_delimiter'],\n",
    "    is_only_first_answer_word=True,\n",
    "    max_answer_time_steps=1)\n",
    "vqa_val_y, _ = encode_answers_one_hot(\n",
    "    vqa_val_raw_y, \n",
    "    vqa_word2index_y, \n",
    "    answer_words_delimiter=vqa_train_text_representation['answer_words_delimiter'],\n",
    "    is_only_first_answer_word=True,\n",
    "    max_answer_time_steps=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.model_zoo import word_generator\n",
    "# We are re-using the BlindBOW mode\n",
    "# Please make sure you have run the cell with the class definition\n",
    "# VQA is larger, so we can increase the dimensionality of the embedding\n",
    "vqa_model_config = Config(\n",
    "    textual_embedding_dim=1000,\n",
    "    input_dim=len(vqa_word2index_x.keys()),\n",
    "    output_dim=len(vqa_word2index_y.keys()),\n",
    "    word_generator = word_generator['max_likelihood'])\n",
    "vqa_text_bow_model = BlindBOW(vqa_model_config)\n",
    "vqa_text_bow_model.create()\n",
    "vqa_text_bow_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vqa_text_bow_model.fit(\n",
    "    vqa_train_x, \n",
    "    vqa_train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=10,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we first need to add word_generator to _config (we could have done this before, in the Config constructor)\n",
    "# we use maximum likelihood as a word generator\n",
    "vqa_predictions_answers = vqa_text_bow_model.decode_predictions(\n",
    "    X=vqa_val_x,\n",
    "    temperature=None,\n",
    "    index2word=vqa_index2word_y,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using VQA is unfortunately not that transparent\n",
    "# we need extra VQA object.\n",
    "vqa_vars = {\n",
    "    'question_id':vqa_val_text_representation['question_id'],\n",
    "    'vqa_object':vqa_val_text_representation['vqa_object'],\n",
    "    'resfun': \n",
    "        lambda x: \\\n",
    "            vqa_val_text_representation['vqa_object'].loadRes(x, vqa_val_text_representation['questions_path'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.utils import print_metrics\n",
    "\n",
    "\n",
    "_ = print_metrics.select['vqa'](\n",
    "        gt_list=vqa_val_raw_y,\n",
    "        pred_list=vqa_predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=vqa_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA Language+Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the name for visual features that we use\n",
    "VQA_CNN_NAME='vgg_net'\n",
    "# VQA_CNN_NAME='googlenet'\n",
    "# the layer in CNN that is used to extract features\n",
    "VQA_PERCEPTION_LAYER='fc7'\n",
    "# PERCEPTION_LAYER='pool5-7x7_s1'\n",
    "\n",
    "vqa_train_visual_features = vqa_dp['perception'](\n",
    "    train_or_test='train',\n",
    "    names_list=vqa_train_text_representation['img_name'],\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=VQA_CNN_NAME,\n",
    "    layer=VQA_PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "vqa_train_visual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vqa_val_visual_features = vqa_dp['perception'](\n",
    "    train_or_test='val',\n",
    "    names_list=vqa_val_text_representation['img_name'],\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=VQA_CNN_NAME,\n",
    "    layer=VQA_PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "vqa_val_visual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.core.model_zoo import Config\n",
    "from kraino.core.model_zoo import word_generator\n",
    "\n",
    "# dimensionality of embeddings\n",
    "VQA_EMBEDDING_DIM = 1000\n",
    "# kind of multimodal fusion (ave, concat, mul, sum)\n",
    "VQA_MULTIMODAL_MERGE_MODE = 'mul'\n",
    "\n",
    "vqa_model_config = Config(\n",
    "    textual_embedding_dim=VQA_EMBEDDING_DIM,\n",
    "    visual_embedding_dim=VQA_EMBEDDING_DIM,\n",
    "    multimodal_merge_mode=VQA_MULTIMODAL_MERGE_MODE,\n",
    "    input_dim=len(vqa_word2index_x.keys()),\n",
    "    output_dim=len(vqa_word2index_y.keys()),\n",
    "    visual_dim=vqa_train_visual_features.shape[1],\n",
    "    word_generator=word_generator['max_likelihood'])\n",
    "vqa_text_image_bow_model = VisionLanguageBOW(vqa_model_config)\n",
    "vqa_text_image_bow_model.create()\n",
    "vqa_text_image_bow_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vqa_train_input = [vqa_train_x, vqa_train_visual_features]\n",
    "vqa_val_input = [vqa_val_x, vqa_val_visual_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== Model training\n",
    "vqa_text_image_bow_model.fit(\n",
    "    vqa_train_input, \n",
    "    vqa_train_y,\n",
    "    batch_size=512,\n",
    "    nb_epoch=10,\n",
    "    validation_split=0.1,\n",
    "    show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we first need to add word_generator to _config (we could have done this before, in the Config constructor)\n",
    "# we use maximum likelihood as a word generator\n",
    "vqa_predictions_answers = vqa_text_image_bow_model.decode_predictions(\n",
    "    X=vqa_val_input,\n",
    "    temperature=None,\n",
    "    index2word=vqa_index2word_y,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using VQA is unfortunately not that transparent\n",
    "# we need extra VQA object.\n",
    "vqa_vars = {\n",
    "    'question_id':vqa_val_text_representation['question_id'],\n",
    "    'vqa_object':vqa_val_text_representation['vqa_object'],\n",
    "    'resfun': \n",
    "        lambda x: \\\n",
    "            vqa_val_text_representation['vqa_object'].loadRes(x, vqa_val_text_representation['questions_path'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kraino.utils import print_metrics\n",
    "\n",
    "\n",
    "_ = print_metrics.select['vqa'](\n",
    "        gt_list=vqa_val_raw_y,\n",
    "        pred_list=vqa_predictions_answers,\n",
    "        verbose=1,\n",
    "        extra_vars=vqa_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kraino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of fast experimentations on Visual Turing Test, we have prepared Kraino that builds on top of Keras.\n",
    "In this short section, you will see how to use it from a command line, example by example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kraino on DAQUAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a blind model with a temporal fusion of the question (equivalent of BOW).\n",
    "\n",
    "One era consists of max_epoch epochs, at the end of era we gather some statistics such as the model performance, or we dump the model weights. Since calculating wups scores is slow, we use 5 epoch before we output such information. In the example below we also use 5 eras, so in total we perform 25 epochs.\n",
    "\n",
    "In the example below we monitor wups scores on test set (--metric=wups, --verbosity=monitor_test_metric), but we also use 10% of training data for validation (--validation_split=0.1).\n",
    "__Please remember to pick up the model based on validation set, NOT the test set!___\n",
    "\n",
    "We use one_hot vector representation. As an alternative we could use --word_representation=dense with a pre-trained embedding such as Word2Vec or Glove. You need to download both pre-trained embeddings.\n",
    "\n",
    "The code below may be slow due to WUPS calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --dataset=daquar-triples --model=sequential-blind-temporal_fusion-single_answer --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=2 --verbosity=monitor_test_metric --word_representation=one_hot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should use smaller embedding layer with --embedding_size=500 (500 dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --textual_embedding_size=500 --dataset=daquar-triples --model=sequential-blind-temporal_fusion-single_answer --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=2 --verbosity=monitor_test_metric --word_representation=one_hot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace the temporal by the recurrent fussion (LSTM) with --model=sequential-blind-__reccurent_fusion__-single_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --dataset=daquar-triples --model=sequential-blind-recurrent_fusion-single_answer --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=1 --verbosity=monitor_test_metric --word_representation=one_hot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily replace LSTM by GRU as a question encoder with --text_encoder=gru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --text_encoder=gru --dataset=daquar-triples --model=sequential-blind-recurrent_fusion-single_answer --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=1 --verbosity=monitor_test_metric --word_representation=one_hot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use 1 dimensional CNN to represent questions with --model=sequential-blind-cnn_fusion-single_answer_with_temporal_fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --text_encoder=gru --dataset=daquar-triples --model=sequential-blind-cnn_fusion-single_answer_with_temporal_fusion --temporal_fusion=sum --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=1 --verbosity=monitor_test_metric --word_representation=one_hot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can combine GRU with visual CNN.\n",
    "\n",
    "--model=sequential-multimodal-recurrent_fusion-at_last_timestep_multimodal_fusion-single_answer \n",
    "\n",
    "--multimodal_fusion=mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --text_encoder=gru --dataset=daquar-triples --model=sequential-multimodal-recurrent_fusion-at_last_timestep_multimodal_fusion-single_answer --temporal_fusion=sum --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=1 --verbosity=monitor_test_metric --word_representation=one_hot --multimodal_fusion=mul "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use the above with Resnet (by default it's GoogLeNet) with piece-wise summation. \n",
    "We use parameters: --perception=fb_resnet --perception_layer=l2_res5c-152."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --text_encoder=gru --dataset=daquar-triples --model=sequential-multimodal-recurrent_fusion-at_last_timestep_multimodal_fusion-single_answer --temporal_fusion=sum --validation_split=0.1 --metric=wups --max_epoch=20 --max_era=1 --verbosity=monitor_test_metric --word_representation=one_hot --multimodal_fusion=sum --perception=fb_resnet --perception_layer=l2_res5c-152 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are more possibilities.\n",
    "Unfortunately the documentation is not ready yet, but you are welcome to experiment with different settings.\n",
    "Interestingly, some subsets of the parameters don't go well with each other.\n",
    "\n",
    "To see available models, check kraino/core/model_zoo.py.\n",
    "\n",
    "To see available command-line parameters together with default values, check kraino/utils/parsers.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kraino on VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, however, also work with VQA.\n",
    "\n",
    "We need to switch dataset (--dataset=vqa-real_images-open_ended), metric (--metric=vqa), and add the answer mode (--vqa_answer_mode=single_frequent).\n",
    "Moreover, we also truncate question answer pairs according to 2000 most frequent answers, and --use_whole_answer_as_answer_word as we want to treat answer 'yellow cab' as the answer, and won't split into words.\n",
    "\n",
    "If the following code is too slow, try using BOW (--model=sequential-blind-temporal_fusion-single_answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --dataset=vqa-real_images-open_ended --model=sequential-blind-recurrent_fusion-single_answer --vqa_answer_mode=single_frequent --metric=vqa --max_epoch=10 --max_era=1 --verbosity=monitor_val_metric --word_representation=one_hot --number_most_frequent_qa_pairs=2000 --use_whole_answer_as_answer_word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use Vision + Language model. If there are memory problems try either smaller batches (--batch_size=...), smaller model (--hidden_state_size or --textual_embedding_size), or use BOW model (--model=equential-multimodal-temporal_fusion-single_answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python neural_solver.py --dataset=vqa-real_images-open_ended --model=sequential-multimodal-recurrent_fusion-at_last_timestep_multimodal_fusion-single_answer --vqa_answer_mode=single_frequent --metric=vqa --max_epoch=10 --max_era=1 --verbosity=monitor_val_metric --word_representation=one_hot --number_most_frequent_qa_pairs=2000 --use_whole_answer_as_answer_word --perception=fb_resnet --perception_layer=l2_res5c-152 --multimodal_fusion=mul "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data and Task Understanding.\n",
    " * Try to find by yourself how difficult/easy is to answer questions without looking into images (do this on DAQUAR or VQA). If you are better than our models, that's great. If you are worse, that's fine, our models, even the blind ones, were trained to answer on this particular dataset.\n",
    " * Take a look at more images, and questions. Can you recognise other challenges that machines can potentially face off? Can you classify the new challenges? \n",
    " * Experiment with evaluation measures. For instance Wu-Palmer Similarities with other categories, or look into [WUPS source code](http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py), or check [Consensus](http://datasets.d2.mpi-inf.mpg.de/mateusz15neural_qa/compute_consensus.py). You are also encouraged to check 'Performance Measure' in the [Multiworld paper](http://arxiv.org/pdf/1410.0210v4.pdf), 'Quantifying the Performance of Holistic Architectures in the [Towards Visual Turing Challenge](http://arxiv.org/pdf/1410.8027v3.pdf), and 'Human Consensus' in the [Ask Your Neurons paper](https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf).\n",
    " * You can experiment with New Predictions sections. Particularly, [the last predictions section](#New-Predictions-with-Vision+Language) was quite short. For instance, you can visualise the predictions, similarly to what you did [here](#New-Predictions). Maybe, you can dump a file with predictions, and come up with new conclusions by inspecting it.\n",
    " * Look into [VQA](http://visualqa.org). [Check images, and question answer pairs](http://visualqa.org/browser/). What are the differences between VQA and DAQUAR?\n",
    "* Experiment with the provided code.\n",
    " * More RNN layers, deeper classifiers.\n",
    " * Different RNN models (GRU, LSTM, your own?).\n",
    " * Using the best model found on a validation set. For this you may be willing to pass the [checkpoint callback](http://keras.io/callbacks/#modelcheckpoint) to the fit function as [this example suggests](http://keras.io/callbacks/#example-model-checkpoints).\n",
    " * Recognise and change hyperparameters such as dimensionality of embeddings or the number of hidden units.\n",
    " * Different ways to fuse two modalities (concat, ave, ...). If you use many RNN layers, you can fuse the modalities at different levels.\n",
    " * Investigate different visual features\n",
    "   * vgg_net: fc6, fc7, fc8\n",
    "   * googlenet: loss3-classifier, pool5-7x7_s1\n",
    " * If in needs, please consult with the [official documentation](keras.io).\n",
    "* Experiments with Keras.\n",
    "* Experiments with [Kraino](#Kraino). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Research Opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Global Representation__ So far we have been using so called global representations of the images. Such representations may destroy too much information, and so we should consider a fine-grained alternative. Maybe we should use detections, or attention models. The latter becomes recently quite successful in answering questions about images. However, there is still a hope for global representations if they are trained end-to-end for the task. Recall that our global representation is extracted from CNN trained on different dataset (ImageNet), and for different task (object classification). \n",
    "* __3D Scene Representation__ Most of current approaches, and all neural approaches, are trained on 2D images. However, it seems that some spatial relations such as 'behind' may need 3d representation of the scene. Luckily, DAQUAR is built on [NYU-Depth dataset](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html) that provides both modes (2D images, and 3D depth). The question if such extra information helps remains open.\n",
    "* __Recurrent Neural Networks__ There is disturbingly small gap between BOW and RNN models. As we have seen before, some questions clearly require an order, but such questions at the same time become longer, semantically more difficult, and require better visual understanding of the world. To handle them we may need other RNNs architectures, or better ways of fusing two modalities, or better __Global Representation__.\n",
    "* __Logical Reasoning__ There are few questions that require a bit more sophisticated logical reasoning such as negation. Can Reccurent Neural Networks learn such logical operators? What about compositionality of the language?\n",
    "* __Language + Vision__ There is too small gap between Language Only and Vision + Language models. But clearly, we need pictures to answer questions about images. So what is missing here? Is it due to __Global Representation__, __3D Scene Representation__ or there is something missing in fusing two modalities?\n",
    "* __Learning from Few Examples__ In the Visual Turint Test, many questions are quite unique. But then how the models can generalise to new questions? What if a question is completely new, but its parts have been already observed (compositionality)? Can models guess the meaning of a new word from its context?\n",
    "* __Ambiguities__ How to deal with ambiguities? They are all inherent in the task, so cannot be just ignored, and should be incorporated into question answering methods as well as evaluation metrics. \n",
    "* __Evaluation Measures__ Although we have WUPS and Consensus, both are far from being perfect. Consensus has higher annotation cost for ambiguous tasks, and is unclear how to formally define good consensus measure. WUPS is an ontology dependent, but can we build one complete ontology that covers all cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Visual Turing Test - project webpage: [https://www.d2.mpi-inf.mpg.de/visual-turing-challenge](https://www.d2.mpi-inf.mpg.de/visual-turing-challenge)\n",
    " * Ask Your Neurons - the main inspiration for this tutorial. [https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf](https://www.d2.mpi-inf.mpg.de/sites/default/files/iccv15-neural_qa.pdf)\n",
    " * Multiworld Approach - our first paper on Visual Turing Test and DAQUAR. It also describes a symbolic approach to handle the challenge. [http://arxiv.org/pdf/1410.0210v4.pdf](http://arxiv.org/pdf/1410.0210v4.pdf)\n",
    " * Towards a Visual Turing Challenge - we hope that it's a nice and accessible introduction to the task. [http://arxiv.org/abs/1410.8027](http://arxiv.org/abs/1410.8027). Its more compact version: [http://arxiv.org/abs/1501.03302](http://arxiv.org/abs/1501.03302)\n",
    " * My [talk from the ICCV'15 conference](http://videolectures.net/iccv2015_malinowski_your_neurons/) together with the [slides](http://datasets.d2.mpi-inf.mpg.de/mateusz15neural_qa/ask_your_neurons-slides-low_res.pdf)\n",
    " * VQA - a complementary, large-scale dataset for Visual Question Answering. Project webpage: [http://visualqa.org](http://visualqa.org)\n",
    " * Image Question Answering - another large-scale dataset for Image Question Answering. Project webpage [http://www.cs.toronto.edu/~mren/imageqa/](http://www.cs.toronto.edu/~mren/imageqa/)\n",
    " * Learning to Answer Questions about Images - similar to Ask Your Neurons model. [http://www.hangli-hl.com/uploads/3/4/4/6/34465961/ma_et_al_aaai_2016.pdf](http://www.hangli-hl.com/uploads/3/4/4/6/34465961/ma_et_al_aaai_2016.pdf)\n",
    " * A nice introduction to VQA with working source codes, also in Keras. [https://avisingh599.github.io/deeplearning/visual-qa/](https://avisingh599.github.io/deeplearning/visual-qa/)\n",
    " * In the Tutorial we use the [facebook residual net features](http://torch.ch/blog/2016/02/04/resnets.html)\n",
    " * DAQUAR uses NYU-Depth images, for more information please take a look [here](http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 25.04.2016: Added residual net features, more on Kraino, made it publicly available\n",
    "1. 20.03.2016: 2nd Summer School on Integrating Vision & Language: Deep Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
